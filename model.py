'''This function updates the model and shows the snapshot of it's input'''import numpy as npimport picklefrom sklearn import preprocessingimport tensorflow as Timport time##   Model parameters:# -     D:  peop2vec dimensionalityD = 100# - C_neg:  number of negative samples accompanying each personC_neg   = 1##  Learning parameters:#       initial  learning rate,  definitely larger than necessarylearning_rate_0 = 100.0regularizer = 1E-16# - N_of_epochs: optimization steps are repeated for  N_epochs timesN_of_epochs = 2# - MsgInterval:MsgInterval = 100000# - Nopt: optimization for each stage is repeated for Nopt times:N_opt=1def initModelAttr(Object, AttrName):    try:        LoadedItem = pickle.load(open(AttrName +'.p','r'))  # in case the model already exists        if AttrName!='Users_dict':            D = LoadedItem.shape[1]    except:        if AttrName!='Users_dict':            LoadedItem = np.array([])        else:            LoadedItem = {}    setattr(Object, AttrName, LoadedItem)def optimizeNewContext(Model, ListOfUserNames):     #updates vecs present in newly added context    Vc_sampled          = np.zeros([1, D])    U_sampled           = np.zeros([1, D])    U_neg_mtx_sampled   = np.zeros([C_neg, D])    # workflow graph setup    # internal vector of a person sampled    Vc          = T.Variable(T.zeros([1, D]))    # external vector of single context person    U           = T.Variable(T.zeros([1, D]))    # matrix of negative context external vectors    U_neg_mtx           = T.Variable(T.zeros([C_neg, D]))    Vc_plchldr              = T.placeholder(T.float32)    U_plchldr               = T.placeholder(T.float32)    U_neg_mtx_plchldr       = T.placeholder(T.float32)    learning_rate_plchldr   = T.placeholder(T.float32)    Score   =   -T.log(T.sigmoid(T.matmul(Vc, T.transpose(U))) + T.constant(regularizer, shape= [1, 1] ))+ \            T.reduce_sum(            -T.log(T.sigmoid(-T.matmul(Vc, T.transpose(U_neg_mtx))) + T.constant(regularizer,  shape= [1, C_neg]))            )    assign_step_Vc          = Vc.assign(Vc_plchldr)    assign_step_U           = U.assign(U_plchldr)    assign_step_U_neg_mtx   = U_neg_mtx.assign(U_neg_mtx_plchldr)    opt_step = T.train.GradientDescentOptimizer(learning_rate_plchldr).minimize(Score)    S = T.Session()    S.run(T.initialize_all_variables())    Nexecutions = 0    #calculate the number of optimization executions    for epochCnt in range(N_of_epochs):            Slen= len(ListOfUserNames)            if Slen>1 and len(Model.Users_dict)-len(ListOfUserNames)>C_neg:                for CentralWordID in range(Slen):                    for contextWordID in range(1, CentralWordID)+ range(CentralWordID+1, Slen):                        Nexecutions+=1    print 'We shall execute optimization procedure for ', Nexecutions, ' times'    ExecutionsCnt = 0    learning_rate = learning_rate_0    t0 = time.time()    epochScore_functional = []  #epochScore calculated as sum of functional values    for epochCnt in range(N_of_epochs):        epochScore_functional.extend([0])        Slen= len(ListOfUserNames)        if Slen>1 and len(Model.Users_dict)-len(ListOfUserNames)>C_neg:            #t2 = t0            for centralWordID in range(Slen):                #each pair of words is treated twice, with context and central words interchanged                for contextWordID in range(1, centralWordID)+ range(centralWordID+1, Slen):                    Vc_sampled_ind = Model.Users_dict[ListOfUserNames[centralWordID]]                    U_sampled_ind  = Model.Users_dict[ListOfUserNames[contextWordID]]                    Vc_sampled     = np.array([Model.Vmtx[Vc_sampled_ind, :]])                    U_sampled      = np.array([Model.Umtx[U_sampled_ind, :]])                    U_neg_mtx_sampled_inds  = []                    cnt=0                    while cnt<(C_neg-1):                        rand_ind = random.randint(0,len(Model.Users_dict)-1)                        if not Model.Users_dict.keys()[rand_ind] in ListOfUserNames:                            U_neg_mtx_sampled_inds.extend([rand_ind])                            cnt+=1                    U_neg_mtx_sampled = np.empty([C_neg, D])                    cnt=0                    for ind in U_neg_mtx_sampled_inds:                        U_neg_mtx_sampled[cnt,:]       = Model.Umtx[ind,:]                    S.run(assign_step_Vc, feed_dict={Vc_plchldr: Vc_sampled})                    S.run(assign_step_U_neg_mtx, feed_dict={U_neg_mtx_plchldr: U_neg_mtx_sampled})                    S.run(assign_step_U, feed_dict={U_plchldr: U_sampled})                    OldScore= S.run(Score)                    epochScore_functional[epochCnt]+= float(OldScore)                    NewScore = np.inf                    learning_rate *= 2                    while NewScore>OldScore:                        S.run(assign_step_Vc, feed_dict={Vc_plchldr: Vc_sampled})                        S.run(assign_step_U, feed_dict={U_plchldr: U_sampled})                        S.run(assign_step_U_neg_mtx, feed_dict={U_neg_mtx_plchldr: U_neg_mtx_sampled})                        for j in range(N_opt):                            S.run(opt_step, feed_dict= {learning_rate_plchldr: learning_rate})                        NewScore= S.run(Score)                        learning_rate /= 2                    if ExecutionsCnt%MsgInterval==0:                        print 'Epoch ', (epochCnt+1), 'of ', N_of_epochs, '. The scores of completed epochs =', epochScore_functional[:epochCnt]                        print 'learning rate =', learning_rate                        #print 'Score after optimization = ', S.run(Score)                        #print 'Score difference = ', S.run(Score) - OldScore                    #print 'Optimization step is done for ', time.time()-t0,' secs.'                    #t0 = time.time()                    # print 'After Learning', 30*'='                    # print 'Vc=', preprocessing.normalize(S.run(Vc))                    # print 'U=',  preprocessing.normalize(S.run(U))                    # print 'Vc*U=', np.dot(preprocessing.normalize(S.run(Vc)), np.transpose(preprocessing.normalize(S.run(U))))                    # saving the results back into vec matrices                    Model.Vmtx[Vc_sampled_ind, :] = preprocessing.normalize(S.run(Vc))                    Model.Umtx[U_sampled_ind, :]  = preprocessing.normalize(S.run(U))                    #print(U_neg_mtx_sampled_inds)                    Model.Umtx[U_neg_mtx_sampled_inds, :] = preprocessing.normalize(U_neg_mtx_sampled)                    #print 'Optimized results are saved, for ', time.time()-t0,' secs.'                    #t0 = time.time()                    ExecutionsCnt+=1                    if ExecutionsCnt%MsgInterval==0:                        t1= time.time()                        print ExecutionsCnt,' optimization steps of total ', Nexecutions,' were taken'                        print 'Time elapsed: ', (t1-t0)/3600 ,'hrs'                        print (Nexecutions - ExecutionsCnt)*(t1-t0)/(3600*ExecutionsCnt), ' hrs left'                        print 100*'_'    S.close()class Model:    def __init__(self):        for Attr in ['Vmtx', 'Umtx', 'Users_dict']:            initModelAttr(self, Attr)    def modelUpdate(self, ListOfUserNames):        for User in ListOfUserNames:            if not self.Users_dict.has_key(User):                self.Users_dict[User]    = len(self.Users_dict)                InitRandomVec       = np.random.rand(1, D) - np.random.rand(1, D)                if len(self.Vmtx)==0:                    self.Vmtx = np.array(InitRandomVec)                else:                    self.Vmtx = np.append(self.Vmtx, np.array(InitRandomVec), axis=0)                InitRandomVec       = np.random.rand(1, D) - np.random.rand(1, D)                if len(self.Umtx)==0:                    self.Umtx = InitRandomVec                else:                    self.Umtx = np.append(self.Umtx, InitRandomVec, axis=0)        optimizeNewContext(self, ListOfUserNames)